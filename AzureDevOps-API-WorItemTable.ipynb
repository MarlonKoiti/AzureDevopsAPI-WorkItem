{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2afa8136-a83a-4e5b-8831-6d941b75e29a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#References:\n",
    "## https://pyodata.readthedocs.io/en/latest/usage/querying.html\n",
    "## Documentation: https://learn.microsoft.com/en-us/rest/api/azure/devops/dashboard/dashboards/list?source=recommendations&view=azure-devops-rest-7.0&tabs=HTTP\n",
    "## Documentation: https://learn.microsoft.com/en-us/azure/devops/boards/queries/wiql-syntax?view=azure-devops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73eb6102-3633-40ee-9017-12abc1185cf4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fc03aa0-98f2-4989-9931-48aaff213f0c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Import Auth Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729be828-f19a-47b5-b582-e88eabca4cd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [\"df_lista_id\"], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": {
        "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [\"df_lista_id\"], \"version\": \"0.0.1\"}",
        "text/plain": ""
       },
       "datasetInfos": [],
       "executionCount": null,
       "metadata": {
        "kernelSessionId": "c3056f65-70cfd53a5516849f79c86b98"
       },
       "removedWidgets": [],
       "type": "mimeBundle"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ref https://learn.microsoft.com/pt-br/rest/api/azure/devops/?view=azure-devops-rest-7.1 < Reference doc difficult to interpret\n",
    "#ref https://stackoverflow.com/questions/60341728/is-there-a-way-to-call-azure-devops-via-python-using-requests < How to make auth token work. There is a trick to add \":\" at the beginning of the token before the encode\n",
    "#ref https://learn.microsoft.com/en-us/azure/devops/extend/develop/work-with-urls?view=azure-devops&tabs=http < Explains a little how to get the urls of the desired objects\n",
    "#ref https://abhijitjana.net/2020/04/11/exploring-azure-devops-apis/ < test the strategies in this post\n",
    "## https://stackoverflow.com/questions/63021168/get-all-work-items-from-a-project-azure-devops-rest-api < Using WiQl to query workitems in project\n",
    "\n",
    "import pandas as pd ## Json response handling\n",
    "import base64  ## Decode of token autenticator \n",
    "import requests  ## Requests a API \n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType #Creation of schema\n",
    "\n",
    "pat= ''  ## token \n",
    "authorization = str(base64.b64encode(bytes(':'+pat, 'ascii')), 'ascii')\n",
    "headers = {'Accept': 'application/json', 'Authorization':'Basic' +authorization}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3847436-3d20-4c52-9b0a-95e8bb1d21d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Creating Initial Table Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9eb7ce54-1049-4119-abdd-d9ae05838936",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Get Teams List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca7a540-23aa-4b6d-a612-28968d6f58fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##developing by dependency on Board\n",
    "## https://learn.microsoft.com/en-us/rest/api/azure/devops/core/teams/get-teams?view=azure-devops-rest-7.0&tabs=HTTP\n",
    "organization = \"\"\n",
    "project = \"\"\n",
    "projectId = \"\"\n",
    "  \n",
    "SERVICE_URL = f\"https://dev.azure.com/{organization}/_apis/projects/{projectId}/teams?api-version=7.0\"\n",
    "print (SERVICE_URL)\n",
    "session = requests.Session() \n",
    " \n",
    "params =\"\"  \n",
    "team_response = session.get(SERVICE_URL,headers=headers ,params=params)\n",
    "\n",
    "print (team_response.status_code)\n",
    "team_response= team_response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a70b9b-0e20-46c6-8be6-e98bacdd52a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "team_data = pd.json_normalize(data = team_response['value'], # datadict or list of dicts\n",
    "                            record_path = None, #record_path str or list of str, default None \n",
    "                            meta =[]) # metalist of paths (str or list of str), default None]\n",
    "\n",
    "team_data = team_data.rename(columns={'id':'team_id'})\n",
    "df_spark_team = spark.createDataFrame(team_data)\n",
    "df_spark_team.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5d912e-762f-49ff-a4a2-3c778b17ccb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##start on 2021-07-14 > pick from item 44 of the list onwards  full_date_tuple_list[45:192] [('2021-07-10', '2021-07-15') ...  ('2023-04-05', '2023-04-10')]\n",
    "full_date_tuple_list = []\n",
    "last_date = '2021-01-01'\n",
    "for year in range(2021,2024):\n",
    "  for month in range(1,13,1):\n",
    "    for day in [1,5,10,15,20,25,28]:\n",
    "      print(f'{year}-{str(month).zfill(2)}-{str(day).zfill(2)}')\n",
    "      current_date = str(f'{year}-{str(month).zfill(2)}-{str(day).zfill(2)}')\n",
    "      full_date_tuple_list.append((last_date , current_date) )    \n",
    "      last_date = current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20afc658-6c14-45a8-b2e9-1aef24d04a8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## loop to see if all teams start on the same day\n",
    "## This code generates 2416 duplicates for each ID\n",
    "\n",
    "## Strategy 2 > Update day\n",
    "     ## Need to handle exception if there are more workitems than the API supports\n",
    "     ## Need to have control of dates being executed recorded so as not to lose information\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"id\",StringType(),True), \\\n",
    "    StructField(\"url\",StringType(),True)])\n",
    "\n",
    "headers = {'Content-Type': 'application/json', 'Authorization':'Basic' +authorization}\n",
    "\n",
    "list_reponses_teams_workItems = [] \n",
    "\n",
    "first_run = True\n",
    "\n",
    "for team_id in team_data['id']:\n",
    "  for data_inicio , data_fim in full_date_tuple_list[45:192]:\n",
    "    data_wit_list =  { \"query\": f\"Select [System.Id], [System.Title], [System.State] From WorkItems where [System.ChangedDate] >= '{data_inicio}' and [System.ChangedDate] < '{data_fim}' order by  [System.CreatedDate] asc   \" }   \n",
    "    response_wit_list = session.post(f'https://dev.azure.com/-/--organization--/{team_id}/_apis/wit/wiql?api-version=7.0',headers=headers ,json=data_wit_list)\n",
    "    print (response_wit_list.status_code)\n",
    "    response_wit_list = response_wit_list.json()\n",
    "    list_reponses_teams_workItems.append( (team_id, team_data[team_data['id']==team_id]['name'], data_inicio , data_fim ))\n",
    "    list_reponses_teams_workItems.append(response_wit_list)\n",
    "\n",
    "  work_item_list = [] \n",
    "  for i in range(1,len(list_reponses_teams_workItems),2):\n",
    "    work_item_list.append(list_reponses_teams_workItems[i]['workItems'])\n",
    "  work_item_list_filtered = [x for x in work_item_list if x != []]\n",
    "  work_item_list_flattened = list(chain(*work_item_list_filtered))\n",
    "  # Create DF based on list \n",
    "  df_tabela =spark.createDataFrame(data=work_item_list_flattened ,schema =schema )\n",
    "  ## save to table \n",
    "  if first_run:\n",
    "    df_tabela.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"\")\n",
    "    first_run = not first_run\n",
    "  else:\n",
    "    df_tabela.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2aef3a31-4a1f-41d4-af30-72ed128a3784",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Create Populated Workitem Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f36761-61a4-41ce-8b48-8889fd99112b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## get list of ids \n",
    "df_spark_min_max_id = spark.sql(f'SELECT Max(id) as max_id , min(id) as min_id  FROM ')\n",
    "df_min_max_id = df_spark_min_max_id.toPandas()\n",
    "#df_min_max_id\n",
    "\n",
    "incremento = 100 ## Theoretically it would be 200 ids per request, but I chose half to not get the process stuck in the request\n",
    "id_start = 104 # to test\n",
    "id_end =  df_min_max_id['max_id'].values[0]\n",
    "id_curr = id_start\n",
    "id_next = int(id_start) + int(incremento) \n",
    "\n",
    "dict_colunas_workitem = {\n",
    "'id':'workitem_id',\n",
    "'rev':'rev',\n",
    "'url':'url',\n",
    "'fields.System.IterationPath':'IterationPath',\n",
    "'fields.System.WorkItemType':'type',\n",
    "'fields.System.State':'state',\n",
    "'fields.System.Reason':'reason',\n",
    "'fields.System.AssignedTo.id':'AssignedTo_id',\n",
    "'fields.System.AssignedTo.uniqueName':'AssignedTo_uniqueName',\n",
    "'fields.System.CreatedDate':'CreatedDate',\n",
    "'fields.System.CreatedBy.id':'CreatedBy_id',\n",
    "'fields.System.CreatedBy.uniqueName':'CreatedBy_uniqueName',\n",
    "'fields.System.ChangedDate':'ChangedDate',\n",
    "'fields.System.ChangedBy.id':'ChangedBy_id',\n",
    "'fields.System.ChangedBy.uniqueName':'ChangedBy_uniqueName',\n",
    "'fields.System.CommentCount':'CommentCount',\n",
    "'fields.System.Title':'Title',\n",
    "'fields.Microsoft.VSTS.Common.StateChangeDate':'StateChangeDate',\n",
    "'fields.Microsoft.VSTS.Common.ActivatedDate':'ActivatedDate',\n",
    "'fields.Microsoft.VSTS.Common.ActivatedBy.id':'ActivatedBy_id',\n",
    "'fields.Microsoft.VSTS.Common.ActivatedBy.uniqueName':'ActivatedBy_uniqueName',\n",
    "'fields.Microsoft.VSTS.Common.ClosedDate':'ClosedDate',\n",
    "'fields.Microsoft.VSTS.Common.ClosedBy.id':'ClosedBy_id',\n",
    "'fields.Microsoft.VSTS.Common.ClosedBy.uniqueName':'ClosedBy_uniqueName',\n",
    "'fields.Microsoft.VSTS.Common.Priority':'Priority',\n",
    "'fields.Microsoft.VSTS.TCM.AutomationStatus':'AutomationStatus',\n",
    "'fields.Custom.Environment':'Environment',\n",
    "'fields.Custom.TestType':'TestType',\n",
    "'fields.Custom.AutomatedLayer':'AutomatedLayer'}\n",
    "\n",
    "column_list_target_table = ['workitem_id', 'rev', 'url', 'IterationPath', 'type', 'state', 'reason', 'AssignedTo_id', 'AssignedTo_uniqueName', 'CreatedDate', 'CreatedBy_id', 'CreatedBy_uniqueName', 'ChangedDate', 'ChangedBy_id', 'ChangedBy_uniqueName', 'CommentCount', 'Title', 'StateChangeDate', 'ActivatedDate', 'ActivatedBy_id', 'ActivatedBy_uniqueName', 'ClosedDate', 'ClosedBy_id', 'ClosedBy_uniqueName','TestType', 'Environment', 'AutomationStatus', 'AutomatedLayer', 'Priority']\n",
    "\n",
    "\n",
    "for work_item_id_list in range(int(id_start),int(id_end),int(incremento)):\n",
    "  df_spark_lista_id = spark.sql(f'SELECT id  FROM database.tablehere where id between {id_curr} and {id_next} order by id ')\n",
    "  df_lista_id = df_spark_lista_id.toPandas()\n",
    "  id_list = df_lista_id['id']\n",
    "  id_list = id_list.values.tolist()\n",
    "  id_list_string =  \",\".join([x  for x in id_list] )  \n",
    "  #id_list_string\n",
    "  params_wit = { } \n",
    "  session = requests.Session()\n",
    "  response_wit_id = session.get(f'https://dev.azure.com///_apis/wit/workitems?ids={id_list_string}&api-version=7.0',headers=headers ,params=params_wit)\n",
    "  print (response_wit_id.status_code)\n",
    "  response_wit_id = response_wit_id.json()\n",
    "  df_workitem_json_response = pd.DataFrame.from_dict(response_wit_id)\n",
    "  workitem_data = pd.json_normalize(data = response_wit_id['value'], # datadict or list of dicts\n",
    "                            record_path = None, #record_path str or list of str, default None \n",
    "                            meta =['System.TeamProject', 'System.WorkItemType']) # metalist of paths (str or list of str), default None\n",
    "##Select desired columns and rename \n",
    "  df_workitem = workitem_data.copy()\n",
    "#rename columns\n",
    "  df_workitem = df_workitem.rename(columns=dict_colunas_workitem) #= df_workitem [['']]\n",
    "\n",
    "##Validate which columns exist\n",
    "  column_list_current = df_workitem.columns.values.tolist()\n",
    "  column_list_insert = [x for x in column_list_current if x in column_list_target_table] ##put in column list and insert only existing columns in dataframe and destination table\n",
    "  #select columns \n",
    "  df_workitem= df_workitem[column_list_insert]\n",
    "## Transform to SparkDataframe \n",
    "  df_spark_workitem = spark.createDataFrame(df_workitem)\n",
    "  df_spark_workitem.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e472dede-3e26-4e76-886e-69bd1dd3fd7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark_workitem.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "AzureDevOps-API-WorItemTable",
   "notebookOrigID": 2470808106854365,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
